---
title: "Final_Project"
author: "R.Riddell"
date: "17/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(janitor)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)


```


```{r}
red_wine <- read_csv("winequality-red.csv")
white_wine <- read_csv("winequality-white.csv")
```
## Data Structure
There is two datasets that will be analysed by the selection of models. These datasets both relate to wine and contain 11 matching features (predictors) which will use to predict the quality (repsonse). The red wine dataset contains 1599 observations and the white wine dataset contains 4898 observations.
```{r data structure}
dim(red_wine)
dim(white_wine)

glimpse(red_wine)
glimpse(white_wine)

# remove spaces from col names
red_wine <- clean_names(red_wine)
white_wine <- clean_names(white_wine)


```

```{r NA's eval=FALSE, include=FALSE}
sum(is.na(red_wine))
sum(is.na(white_wine))
```

## Distribution of Quality (response varaible)
When looking at the distrubtion of quality in the red wine data we see a responable normal distrubtion, there are very few total observations with a quality rating of 3,4 and 8. This may be a factor when looking to predict quality as the datset imbalance may skew results. The white eine datset has a more normal distrbution with a mroe gradual step down from the peak of the histogram. The number of observations with qaulity of 3 and 9 are extremly low, while 4 and 8 have a relatively low number of observations.
```{r eval=FALSE, include=FALSE}
ggplot(red_wine, aes(quality)) + 
  geom_histogram(binwidth = 1, fill = 'navy') +
  ggtitle("Red Wine: Distribution of Quality (response variable)") +
  xlab('Quality') +
  theme_bw()
  
ggplot(white_wine, aes(quality)) + 
  geom_histogram(binwidth = 1, fill = 'navy') +
  ggtitle("White Wine: Distribution of Quality (response variable)") +
  xlab('Quality') +
  theme_bw()
```
## Distribution Of Predictors
When assesing the distributiono f the predictor variables with the use of histograms, there are some similarites across features. In both datasets Density and Ph have a normal distribution, while chlorides, residual sugar and alchol have a non - normal distrubtion and fixed acidity and volatile acidity are seent to be reasonably normal. When comparing the features that differ across the datasets we can see citric acid, free sulfur dioxide, total sulfur dioxide, sulphates appear as a non normal distribution in the red wine data but appear resoanble normal in the white wine data. This leads me to conclude that on the whole the white wine data set has a more normal distrubtion compared to the red wine data. When looking at possible outliers the features that appear to have outliers in both datasets are residual sugar and chlorides, while only in the red wine data does suplahe appera to have obvious outliers. 

```{r eval=FALSE, include=FALSE}
gg_histogram <- function(data,feature){
  ggplot(data, aes(.data[[feature]])) + 
    geom_histogram(fill = 'navy') +
    ggtitle(deparse(substitute(data)) , feature) +
    theme_bw()
}

## establish an object that contains the variables that are either catergorical or have less than 20 unique characters
numeric_vars <- red_wine  %>% 
  select(where(is_numeric),- quality) %>% 
  names(.)  

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_histogram(red_wine, r_numeric_vars[i]))
}

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_histogram(white_wine, w_numeric_vars[i]))
}
```


## Scatterplots Predictors vs Reponse
Comparing the predictors to wuality in scatterplots was somewhat challenging as the quality figure in an integer in nature, to aid with this while plotting the function geom_jitter() was used to get a better understanding on where the repsonses fall on the predictors scale. When visually assesing the linear relatioship it seems both datasets have a similar pattern on matching features. It could be said volatile acidity and alchohol have moderate linear relationship, while all other features have a poor linear relationship. This leads me to conclude that any linear modelling technique may not be appropriate on either of these datasets. 
```{r eval=FALSE, include=FALSE}
## a function to create boxplots
gg_scatterplot <- function(data, feature){
  ggplot(data = data, aes(y = quality, x = .data[[feature]])) +
    geom_jitter() +
    geom_smooth(method = 'lm') +
    ggtitle(deparse(substitute(data)), feature) +
    ylab(paste('Quality')) +
    theme(legend.position = "none", 
          axis.text.x = element_text(angle = 45))
}
## establish an object that contains the variables that are either catergorical or have less than 20 unique characters
## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_scatterplot(red_wine, r_numeric_vars[i]))
}

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_scatterplot(white_wine, w_numeric_vars[i]))
}

red_wine %>% 
  select(is.numeric) %>% 
  corrr::correlate() %>% 
  corrr::focus(quality) %>%
  arrange(quality) 

white_wine %>% 
  select(is.numeric) %>% 
  corrr::correlate() %>% 
  corrr::focus(quality) %>%
  arrange(quality)

```


## Boxplots for predictors
To assess the suitableability of classification models the distribution based on each level of the response varaibles vs each preditor was assesed through boxplots. This was done to assess how each predictor affects each level of the reponse varible and if the levels of the repsonse variable can be used to clearly discrimnant the quality. Through this visual assesment it seems some of the predictors show a clear difference in their median values and could be important festures when it comes to classfication prediction. Across both datasets volatile acidity, density and alcohol appear to have a consistently different median values which may mean they are able to descrimnante classes and imoprve predictions. In addition to this in the red wine data sulphates and total sulfur dioxide appear to be good predicttors, while in the white wine data chlorides and free sulfur dioxide appear to make good predictors. 
```{r eval=FALSE, include=FALSE}
## a function to create boxplots
gg_boxplot <- function(data, feature){
  ggplot(data = data, aes(x = quality, y = .data[[feature]], group = quality)) +
    geom_boxplot() +
    xlab(paste('Quality')) +
    ggtitle(deparse(substitute(data)), feature) +
    theme(legend.position = "none", 
          axis.text.x = element_text(angle = 45))
}

numeric_vars <- red_wine  %>% 
  select(where(is_numeric),- quality) %>% 
  names(.)  

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_boxplot(red_wine, r_numeric_vars[i]))
}

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_boxplot(white_wine, w_numeric_vars[i]))
}


```

```{r split data}
set.seed(345)

inTrain_red <- createDataPartition(y = red_wine$quality, p = 0.8, list = F)

red_training <-  red_wine %>% 
  slice(inTrain_red)

red_testing <-  red_wine %>% 
  slice(-inTrain_red)

inTrain_white <- createDataPartition(y = white_wine$quality, p = 0.8, list = F)

white_training <-  white_wine %>% 
  slice(inTrain_white)

white_testing <-  white_wine %>% 
  slice(-inTrain_white)

rm(inTrain_red, inTrain_white)
```


```{r control obj}
control_obj_regression <- trainControl(method = "cv", 
                                       number = 5)


control_obj_classification <- trainControl(
  #method = 'repeatedcv',
  method = "cv",
  number = 5,
  #repeats = 5
  savePredictions = "final",
  classProbs = T,
  summaryFunction = defaultSummary
)

```

## Multiple Linear Regression Model (MLM)
The data sets were intialled modelled using a standard multiple linear regression, both were then tuned to see if there was an improvement by using a stepwise linear model. The stepwise method for fitting a regression model is where each varaible is selected based on it signifincance. in relation to both wine dataset the stepwise tuning resulting in a very small increase to the adjusted R2 value. When comparing the adjusted R-sqaure values from both models we see that the linear model is better at representing the red wine data, resulting in an R-sqaure of 0.35. Where the R-sqaure value from the white wine data was 0.28. When applied to the testing data the red wine data returns an RMSE value of 0.68 and the white wine data returns 0.75. In combintaion of both the R-sqaure and RMSE values we can see the MLM is a better model for the red wine data. I dont think a linear model particularly suits either dataset as the R-sqaure values are quite low therfore suggesting the model does not represent the data well. something that could hinder the linear model is the reponse value being an interger, when the model is looking to predict it will use floats and therefore smooths out the discrmepanciey from one interger to another. It was also seen through the the scatter plots there was large amount of variance and no feature returned a visually strong correlation. 
```{r MLM}
library(MASS)
set.seed(345)

red.mlm_mdl <- train(quality ~ . , data = red_training, 
                    method = 'lm',
                    trControl = control_obj_regression,
                    trace = F)
red.mlm_mdl
summary(red.mlm_mdl)
# Insignificant values: fixed_acidity,citric_acid,residual_sugar,density

red.mlm_mdl.tuned <- train(quality ~ . , data = red_training, 
                method = 'lmStepAIC',
                trControl = control_obj_regression,
                trace = F)
red.mlm_mdl.tuned
summary(red.mlm_mdl.tuned)  

# applying to testing data
red_testing <-  red_testing %>% 
  mutate(predictions = predict(red.mlm_mdl.tuned, newdata = red_testing))

## calculting the RMSE amd R squared of the linear model on the testing data
red.mlm_mdl.tuned.metrics <- data.frame(
    dataset = 'Red Wine',
  RMSE =RMSE(pred = red_testing$predictions,
       obs = red_testing$quality),
  R2 = R2(pred = red_testing$predictions,
     obs = red_testing$quality)
)
## plotting the predictions against the actual score margin from the testing data
ggplot(red_testing, aes(predictions, quality)) +
  geom_point(colour = '#1111FF', alpha=0.7) +
  geom_abline(colour = "red", linetype = "dashed")

#-------------------------------------------------------------------------------------#
white.mlm_mdl <- train(quality ~ . , data = white_training, 
                      method = 'lm',
                      trControl = control_obj_regression,
                      trace = F)
white.mlm_mdl
summary(white.mlm_mdl)
# Insignificant values: citric_acid,chlorides,total_sulfur_dioxide 
white.mlm_mdl.tuned <- train(quality ~ . , data = white_training, 
                method = 'lmStepAIC',
                trControl = control_obj_regression,
                trace = F)
white.mlm_mdl.tuned
summary(white.mlm_mdl.tuned)  

# applying to testing data
white_testing <-  white_testing %>% 
  mutate(predictions = predict(white.mlm_mdl.tuned, newdata = white_testing))

## calculting the RMSE amd R squared of the linear model on the testing data
white.mlm_mdl.tuned.metrics <- data.frame(
  dataset = 'White Wine',
  RMSE = RMSE(pred = white_testing$predictions,
       obs = white_testing$quality),
  R2 = R2(pred = white_testing$predictions,
     obs = white_testing$quality)
)
## plotting the predictions against the actual score margin from the testing data
ggplot(white_testing, aes(predictions, quality)) +
  geom_point(colour = '#1111FF', alpha=0.7) +
  geom_abline(colour = "red", linetype = "dashed")

rbind(red.mlm_mdl.tuned.metrics, white.mlm_mdl.tuned.metrics)

detach("package:MASS")
rm(red.mlm_mdl,red.mlm_mdl.tuned.metrics,white.mlm_mdl,white.mlm_mdl.tuned.metrics)

```

## Penalised regression models 
Both datasets contain multiple varaibles so to test how the dataset viewed non signinfnat data it was subjected to penalised regression modelling, namely Ridge, Lasso and elastic. As observerd with the MLM the red wine data is slightly better represented with a linear model, this can also be observed with the results of the linear model. After processing the data through the penalised model we can observe very similar results across the three models. Both the red and white wine data are best modelled by the Ridge regression but to see thie improvement we must go to the third decimal place. The difference between the penalised model and MLM is also very small with a slightly better RMSE, this potenatilly means that excess features is not what is affecting the original MLM and instead the data is badly represented by a linear model. 
```{r Penalised Models}
set.seed(345)
lambda <- 10^seq(-3,3,length=100)

# Ridge
red_ridge <- train(
  quality~., data=red_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=0, lambda=lambda)
)

coef(red_ridge$finalModel, red_ridge$bestTune$lambda)
predictions <- red_ridge %>% predict(red_testing)

red_ridge.metrics <- data.frame(
  dataset = 'Red Wine',
  model = 'Ridge',
  RMSE = RMSE(predictions, red_testing$quality),
  Rsquare = R2(predictions, red_testing$quality)
)

#lasso
set.seed(123)
red_lasso <- train(
  quality~., data=red_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=1, lambda=lambda)
)
  
coef(red_lasso$finalModel, red_lasso$bestTune$lambda)
predictions <- red_lasso %>% predict(red_testing)

red_lasso.metrics <- data.frame(
  dataset = 'Red Wine',
  model = 'Lasso',
  RMSE = RMSE(predictions, red_testing$quality),
  Rsquare = R2(predictions, red_testing$quality)
)

#elastic
set.seed(123)
red_elastic <- train(
  quality~., data=red_training, method="glmnet", 
  trControl=control_obj_regression,
)

coef(red_elastic$finalModel, red_elastic$bestTune$lambda)
predictions <- red_elastic %>% predict(red_testing)

red_elastic.metrics <- data.frame(
  dataset = 'Red Wine',
  model = 'elastic',
  RMSE = RMSE(predictions, red_testing$quality),
  Rsquare = R2(predictions, red_testing$quality)
)

#caret compare all of ridge, lasso, elastic net: 
#choose best with smallest median or mean RMSE
models <- list(red_ridge=red_ridge, red_lasso=red_lasso, red_elastic=red_elastic)
resamples(models) %>% summary(metric="RMSE")

#-------------------------------------------------------------------------------------#

# Ridge
white_ridge <- train(
  quality~., data=white_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=0, lambda=lambda)
)

coef(white_ridge$finalModel, white_ridge$bestTune$lambda)
predictions <- white_ridge %>% predict(white_testing)

white_ridge.metrics <- data.frame(
  dataset = 'White Wine',
  model = 'Ridge',
  RMSE = RMSE(predictions, white_testing$quality),
  Rsquare = R2(predictions, white_testing$quality)
)

#lasso
set.seed(123)
white_lasso <- train(
  quality~., data=white_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=1, lambda=lambda)
)
  
coef(white_lasso$finalModel, white_lasso$bestTune$lambda)
predictions <- white_lasso %>% predict(white_testing)

white_lasso.metrics <- data.frame(
  dataset = 'White Wine',
  model = 'Lasso',
  RMSE = RMSE(predictions, white_testing$quality),
  Rsquare = R2(predictions, white_testing$quality)
)

#elastic
set.seed(123)
white_elastic <- train(
  quality~., data=white_training, method="glmnet", 
  trControl=control_obj_regression,
)

coef(white_elastic$finalModel, white_elastic$bestTune$lambda)
predictions <- white_elastic %>% predict(white_testing)

white_elastic.metrics <- data.frame(
  dataset = 'White Wine',
  model = 'Elastic',
  RMSE = RMSE(predictions, white_testing$quality),
  Rsquare = R2(predictions, white_testing$quality)
)


#caret compare all of ridge, lasso, elastic net: 
#choose best with smallest median or mean RMSE
models <- list(red_ridge=red_ridge, red_lasso=red_lasso, red_elastic=red_elastic,
               white_ridge=white_ridge, white_lasso=white_lasso, white_elastic=white_elastic)
resamples(models) %>% summary(metric="RMSE")

rbind(red_ridge.metrics,red_lasso.metrics,red_elastic.metrics,
      white_ridge.metrics,white_lasso.metrics,white_elastic.metrics)

rm(red_elastic,red_elastic.metrics,red_lasso,red_lasso.metrics,red_ridge,red_ridge.metrics,
   white_elastic,white_elastic.metrics,white_lasso,white_lasso.metrics,white_ridge,white_ridge.metrics,
   lambda, predictions)
```
## Decsion Tree Model
Intially both datsets were processed using an ANOVA method which looks to minimise the variance at each node. This resulted in the red wine data creating a decsion tree of 3 features, splitting on alcohol, sulphates and volatile acidity. The white wine data created a tree split on 4 features, alcohol, free sulfur dioxide and volatile acidity. To attempt to tune the model it was subjected to multiple complexity parameters, in both cases the metrics (RMSE, R-Sqaure) were improved when the complexity paramter was increased and the decsion tree included more features. When comparing the two tuned trees on the testing data the red wine data shows an improved RMSE compared to the white wine data but the white wine data had a better R-sqaure value. As the tuned decsion trees are much more complex they could be prone overfitting and be incorrect with new data, but as we see an improvement on RMSE metrics of the trained tree when compared to the testing data I do not belive the trees are overfit. When looking at important features of the datasets teh red wine data seemed more heavlity reliant on the first three features, while the whiet wine data had a more even step down of feature imporatnce. The red wine data completly ignored the residual sugar feature while the white wine ignored the sulphates. The white wine dataset performance is very similar to the MLM model, while it seems to fit the red wine dataset worse as a R-sqaure value of 0.23 is returned compared to that of 0.34 in the MLM model. Overall the desion tree performed poorly for regression and it may be due to the large amount of value overlap we can see when looking at the boxplots in many of the variables.

```{r DT}
set.seed(345)


red_tree <-  rpart(quality ~.,
                    data = red_training,
                    method = "anova")
rpart.plot(red_tree)

predictions = predict(red_tree, newdata = red_testing)

red_tree.metrics <- data.frame(
  dataset = 'Red Wine',
  Model = 'Tree',
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

red_tree.tuned <- train(quality ~.,
                   data = red_training,
                   method = "rpart",
                   trControl = control_obj_regression,
                   tuneGrid = expand.grid(cp = seq(0.001,0.02,0.001)))
red_tree.tuned
plot(red_tree.tuned)
rattle::fancyRpartPlot(red_tree.tuned$finalModel, sub = "")
plot(varImp(red_tree.tuned))


predictions = predict(red_tree.tuned, newdata = red_testing)

red_tree.tuned.metrics <- data.frame(
  dataset = 'Red Wine',
  Model = 'Tuned Tree',
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

rbind(red_tree.metrics, red_tree.tuned.metrics)

#-------------------------------------------------------------------------------------#

white_tree <-  rpart(quality ~.,
                    data = white_training,
                    method = "anova")
rpart.plot(white_tree)

predictions = predict(white_tree, newdata = white_testing)

white_tree.metrics <- data.frame(
  dataset = "White Wine",
  Model = 'Tree',
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

white_tree.tuned <- train(quality ~ .,
                   data = white_training,
                   method = "rpart",
                   trControl = control_obj_regression,
                   tuneGrid = expand.grid(cp = seq(0.001,0.01,0.001)))
white_tree.tuned
plot(white_tree.tuned)
rattle::fancyRpartPlot(white_tree.tuned$finalModel, sub = "")
plot(varImp(white_tree.tuned))


predictions = predict(white_tree.tuned, newdata = white_testing)

white_tree.tuned.metrics <- data.frame(
  dataset = "White Wine",
  Model = 'Tuned Tree',
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

rbind(white_tree.metrics, white_tree.tuned.metrics)

models <- list(red_tree.tuned=red_tree.tuned,
               white_tree.tuned=white_tree.tuned)
resamples(models) %>% summary(metric="RMSE")

rbind(red_tree.metrics, red_tree.tuned.metrics, white_tree.metrics, white_tree.tuned.metrics)

```
## Random forest Model
The random forest model found the best model fit using 6 trees at each split for the red wine dataset and 3 features at each split for the white wine. As more features were introduced into the model R-sqaured values decreased and RMSE increased. The random forest model returned the lowest RMSE and highest R-Sqaure values for both the red and white wine data. The model had a slightly better fit on the white wine data with a RMSE and R-sqaure of 0.60 and 0.53 against the red wine values that were RMSE = 0.63 and R-sqaure = 0.43. The random forest model may have performed better on the data due to its random nature being able to better understand teh overlap between feature behavour and reposnse outcome. As we saw with the scatter plots some features had very little impact on quality while some features clearly had a stronger relationship. using its many iterations the random forest was able to trial many of the less significant features and understand how those values impact the prediction. This is a signiifncat imprvemnt on the single decison tree as the single tree struggled to caputre the ovaerlal variabibilty of the data while the iterations of teh random forest are better suited at encapsualting that variability.
```{r RF}
set.seed(345)
gridrf <- expand.grid(.mtry = c(1:11))

red.rf_mdl <- train(quality ~ ., data = red_training,
                method = 'rf',
                trControl = control_obj_regression,
                tuneGrid = gridrf)

red.rf_mdl
plot(red.rf_mdl)
plot(varImp(red.rf_mdl))

predictions = predict(red.rf_mdl, newdata = red_testing)

red_rf.metrics <- data.frame(
  dataset = 'Red Wine',
  model = 'Random Forest',
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

#-------------------------------------------------------------------------------------#

white.rf_mdl <- train(quality ~ ., data = white_training,
                method = 'rf',
                trControl = control_obj_regression,
                tuneGrid = gridrf)


white.rf_mdl
plot(white.rf_mdl)
plot(varImp(white.rf_mdl))

predictions = predict(white.rf_mdl, newdata = white_testing)

white_rf.metrics <- data.frame(
  dataset = 'White Wine',
  model = 'Random Forest',
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)


models <- list(red.rf_mdl=red.rf_mdl,
               white.rf_mdl=white.rf_mdl)
resamples(models) %>% summary(metric="RMSE")

rbind(red_rf.metrics, white_rf.metrics)

```
## K Nearest Neighbour Model
The data for both red and white wine was intially processed with the standard knn model with no tuning parameter, when doing this the algorithm will choose a selection of values for k. Using the information from teh first model we are able to get and understaiding of what values of k are likely to create the best result. In order to ensure the model is as good as possible we are able to select values for k with teh algorithm will attempt to use and return the one that offers the best result. The red wine data chose a k of 13 and the tuning showed a steep decrease from the lower values of k until levelling out at around a k of 10. the white wine data chose a k of 16 , showing a similar pattern and levelling out at around 10. This shows the model is looking for a resonable wide set of neighbours to attempt to smooth out some of the variability in the data. Both the tuned model showed a slight improvement when compared to the testing data, with the red wine having a lower RMSE value but the wine data having a higher R-Sqaure. The KNN model performed similar with both datasets, although this model does not represent the data well with an R-sqaure of 0.32 and 0.34 on the red and white wine data respectively.
```{r KNN} 
set.seed(345)

red.knn_mdl <-  train(quality~ ., data = red_training, method = "knn",
                  preProc = c('center', 'scale'),
                  trControl = control_obj_regression)
red.knn_mdl
plot(red.knn_mdl)

predictions = predict(red.knn_mdl, newdata = red_testing)

red_knn.metrics <- data.frame(
  dataset = 'Red Wine',
  model = 'KNN',
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

red.knn_mdl.tuned <-  train(quality~ ., data = red_training, method = "knn",
                  preProc = c('center', 'scale'),
                  tuneGrid = data.frame(.k = 1:20),
                  trControl = control_obj_regression)
red.knn_mdl.tuned
plot(red.knn_mdl.tuned)

predictions = predict(red.knn_mdl.tuned, newdata = red_testing)

red_knn.tuned.metrics <- data.frame(
  dataset = 'Red Wine',
  model = 'KNN Tuned',
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

#-------------------------------------------------------------------------------------#

white.knn_mdl <-  train(quality~ ., data = white_training, method = "knn",
                  preProc = c('center', 'scale'),
                  trControl = control_obj_regression)
white.knn_mdl
plot(white.knn_mdl)

predictions = predict(white.knn_mdl, newdata = white_testing)

white_knn.metrics <- data.frame(
  dataset = 'White Wine',
  model = 'KNN',
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

white.knn_mdl.tuned <-  train(quality~ ., data = white_training, method = "knn",
                  preProc = c('center', 'scale'),
                  tuneGrid = data.frame(.k = 1:20),
                  trControl = control_obj_regression)
white.knn_mdl.tuned
plot(white.knn_mdl.tuned)

predictions = predict(white.knn_mdl.tuned, newdata = white_testing)

white_knn.tuned.metrics <- data.frame(
  dataset = 'White Wine',
  model = 'KNN Tuned',
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

models <- list(red.knn_mdl=red.knn_mdl, red.knn_mdl.tuned = red.knn_mdl.tuned,
               white.knn_mdl=white.knn_mdl, white.knn_mdl.tuned = white.knn_mdl.tuned)
resamples(models) %>% summary(metric="RMSE")

rbind(red_knn.metrics, red_knn.tuned.metrics, white_knn.metrics, white_knn.tuned.metrics)

#plot RMSE of reg models
models <- list(red.knn_mdl=red.knn_mdl, red.knn_mdl.tuned = red.knn_mdl.tuned,
               white.knn_mdl=white.knn_mdl, white.knn_mdl.tuned = white.knn_mdl.tuned)
resamples(models) %>% summary(metric="RMSE")
```


```{r regression models} 
resampls <- resamples(list(Red_MLM = red.mlm_mdl.tuned,
                           White_MLM = white.mlm_mdl.tuned,
                           Red_Ridge = red_ridge,
                           White_ridge = white_ridge,
                           red_tree = red_tree.tuned,
                           white_tree = white_tree.tuned,
                           red_rf = red.rf_mdl,
                           white_rf = white.rf_mdl,
                           red_knn = red.knn_mdl.tuned,
                           white_knn = white.knn_mdl.tuned))

## displaying the model perfomance in order to select the best model
gridExtra::grid.arrange(bwplot(resampls, metric = 'RMSE'),
                        bwplot(resampls, metric = 'Rsquared'),
                        ncol = 2)

```

# Classification Models
## Split Data For Classification
```{r RF Classification}
set.seed(345)

red_wine$quality <- as.factor(red_wine$quality)
red_wine$quality <- as.factor(red_wine$quality)

white_wine$quality <- as.factor(white_wine$quality)
white_wine$quality <- as.factor(white_wine$quality)

red_wine <- red_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))
red_wine <- red_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))
white_wine <- white_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))
white_wine <- white_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))


inTrain_red <- createDataPartition(y = red_wine$quality, p = 0.8, list = F)

red_c_training <-  red_wine %>% 
  slice(inTrain_red)

red_c_testing <-  red_wine %>% 
  slice(-inTrain_red)

inTrain_white <- createDataPartition(y = white_wine$quality, p = 0.8, list = F)

white_c_training <-  white_wine %>% 
  slice(inTrain_white)

white_c_testing <-  white_wine %>% 
  slice(-inTrain_white)

```

## Random Forest Model
```{r RF Classification}
set.seed(345)

red_c_training %>% 
  group_by(quality) %>% 
  count()

white_c_training %>% 
  group_by(quality) %>% 
  count()


red_rf_mdl.C <- train(quality ~ . ,
                 data = red_training,
                 method = "rf",
                 trControl = control_obj_classification,
                 #tuneGrid = tune,
                 importance = TRUE, 

                 )
plot(red_rf_mdl.C)
plot(varImp(red_rf_mdl.C))


confusionMatrix(data = red_rf_mdl.C,
                reference = red_c_testing$quality)


white_rf_mdl.C <- train(quality ~ . ,
                 data = white_training,
                 method = "rf",
                 trControl = control_obj_classification,
                 #tuneGrid = tune,
                 importance = TRUE, 
                 )
plot(white_rf_mdl.C)
plot(varImp(white_rf_mdl.C))

confusionMatrix(data = white_rf_mdl.C,
                reference = white_c_testing$quality)


```

## Boosted Model
```{r Boosted}
set.seed(345)

red_gbtree <- train(quality ~.,
                data = red_c_training,
                method = "xgbTree",
                trControl = control_obj_classification)


plot(varImp(red_gbtree))

confusionMatrix(data = gbtree,
                reference = red_c_testing$quality)


white_gbtree <- train(quality ~.,
                data = white_c_training,
                method = "xgbTree",
                trControl = control_obj_classification)


plot(varImp(white_gbtree))

confusionMatrix(data = white_gbtree,
                reference = white_c_testing$quality)

plot(red_gbtree)
```

