---
title: "Final_Project"
author: "R.Riddell"
date: "17/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(janitor)
library(ggplot2)
library(caret)

```


```{r}
red_wine <- read_csv("winequality-red.csv")
white_wine <- read_csv("winequality-white.csv")
```
## Data Structure
There is two datasets that will be analysed by the selection of models. These datasets both relate to wine and contain 11 matching features (predictors) which will use to predict the quality (repsonse). The red wine dataset contains 1599 observations and the white wine dataset contains 4898 observations.
```{r data structure eval=FALSE, include=FALSE}
dim(red_wine)
dim(white_wine)

glimpse(red_wine)
glimpse(white_wine)

# remove spaces from col names
red_wine <- clean_names(red_wine)
white_wine <- clean_names(white_wine)


```
```{r NA's eval=FALSE, include=FALSE}
sum(is.na(red_wine))
sum(is.na(white_wine))
```

## Distribution of Quality (response varaible)
When looking at the distrubtion of quality in the red wine data we see a responable normal distrubtion, there are very few total observations with a quality rating of 3,4 and 8. This may be a factor when looking to predict quality as the datset imbalance may skew results. The white eine datset has a more normal distrbution with a mroe gradual step down from the peak of the histogram. The number of observations with qaulity of 3 and 9 are extremly low, while 4 and 8 have a relatively low number of observations.
```{r eval=FALSE, include=FALSE}
ggplot(red_wine, aes(quality)) + 
  geom_histogram(binwidth = 1, fill = 'navy') +
  ggtitle("Red Wine: Distribution of Quality (response variable)") +
  xlab('Quality') +
  theme_bw()
  
ggplot(white_wine, aes(quality)) + 
  geom_histogram(binwidth = 1, fill = 'navy') +
  ggtitle("White Wine: Distribution of Quality (response variable)") +
  xlab('Quality') +
  theme_bw()
```
## Distribution Of Predictors
When assesing the distributiono f the predictor variables with the use of histograms, there are some similarites across features. In both datasets Density and Ph have a normal distribution, while chlorides, residual sugar and alchol have a non - normal distrubtion and fixed acidity adn volatile acidity are seent to be reasonably normal. When comparing the features that differ across the datasets we can see citric acid, free sulfur dioxide, total sulfur dioxide, sulphates appear as a non normal distribution in the red wine data but appear resoanble normal in the white wine data. This leads me to conclude that on the whole the white wine data set has a more normal distrubtion and may be more suitable to linear modelling techniques compared to the red wine data. When looking at possible outliers the features that appear to have outliers in both datasets are residual sugar and chlorides, while only in the red wine data does suplahe appera to have obvious outliers. 

```{r eval=FALSE, include=FALSE}
gg_histogram <- function(data,feature){
  ggplot(data, aes(.data[[feature]])) + 
    geom_histogram(fill = 'navy') +
    ggtitle(deparse(substitute(data)) , feature) +
    theme_bw()
}

## establish an object that contains the variables that are either catergorical or have less than 20 unique characters
numeric_vars <- red_wine  %>% 
  select(where(is_numeric),- quality) %>% 
  names(.)  

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_histogram(red_wine, r_numeric_vars[i]))
}

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_histogram(white_wine, w_numeric_vars[i]))
}
```


## Scatterplots Predictors vs Reponse
Comparing the predictors to wuality in scatterplots was somewhat challenging as the quality figure in an integer in nature, to aid with this while plotting the function geom_jitter() was used to get a better understanding on where the repsonses fall on the predictors scale. When visually assesing the linear relatioship it seems both datasets have a similar pattern on matching features. It could be said volatile acidity and alchohol have moderate linear relationship, while all other features have a poor linear relationship. This leads me to conclude that any linear modelling technique may not be appropriate on either of these datasets. 
```{r eval=FALSE, include=FALSE}
## a function to create boxplots
gg_scatterplot <- function(data, feature){
  ggplot(data = data, aes(y = quality, x = .data[[feature]])) +
    geom_jitter() +
    geom_smooth(method = 'lm') +
    ggtitle(deparse(substitute(data)), feature) +
    ylab(paste('Quality')) +
    theme(legend.position = "none", 
          axis.text.x = element_text(angle = 45))
}
## establish an object that contains the variables that are either catergorical or have less than 20 unique characters
## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_scatterplot(red_wine, r_numeric_vars[i]))
}

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_scatterplot(white_wine, w_numeric_vars[i]))
}

red_wine %>% 
  select(is.numeric) %>% 
  corrr::correlate() %>% 
  corrr::focus(quality) %>%
  arrange(quality) 

white_wine %>% 
  select(is.numeric) %>% 
  corrr::correlate() %>% 
  corrr::focus(quality) %>%
  arrange(quality)

```


## Boxplots for predictors
To assess the suitableability of classification models the distribution based on each level of the response varaibles vs each preditor was assesed through boxplots. This was done to assess how each predictor affects each level of the reponse varible and if the levels of the repsonse variable can be used to clearly discrimnant the quality. Through this visual assesment it seems some of the predictors show a clear difference in their median values and could be important festures when it comes to classfication prediction. Across both datasets volatile acidity, density and alcohol appear to have a consistently different median values which may mean they are able to descrimnante classes and imoprve predictions. In addition to this in the red wine data sulphates and total sulfur dioxide appear to be good predicttors, while in the white wine data chlorides and free sulfur dioxide appear to make good predictors. 
```{r eval=FALSE, include=FALSE}
## a function to create boxplots
gg_boxplot <- function(data, feature){
  ggplot(data = data, aes(x = quality, y = .data[[feature]], group = quality)) +
    geom_boxplot() +
    xlab(paste('Quality')) +
    ggtitle(deparse(substitute(data)), feature) +
    theme(legend.position = "none", 
          axis.text.x = element_text(angle = 45))
}

numeric_vars <- red_wine  %>% 
  select(where(is_numeric),- quality) %>% 
  names(.)  

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_boxplot(red_wine, r_numeric_vars[i]))
}

## for loop to create all the boxplots
for (i in seq_along(numeric_vars)) {
  print(gg_boxplot(white_wine, w_numeric_vars[i]))
}


```

```{r split data}
set.seed(345)

inTrain_red <- createDataPartition(y = red_wine$quality, p = 0.8, list = F)

red_training <-  red_wine %>% 
  slice(inTrain_red)

red_testing <-  red_wine %>% 
  slice(-inTrain_red)

inTrain_white <- createDataPartition(y = white_wine$quality, p = 0.8, list = F)

white_training <-  white_wine %>% 
  slice(inTrain_white)

white_testing <-  white_wine %>% 
  slice(-inTrain_white)

rm(inTrain_red, inTrain_white, i, r_numeric_vars, w_numeric_vars, gg_boxplot)
```


```{r control obj}
control_obj_regression <- trainControl(method = "cv", 
                                       number = 5)


control_obj_classification <- trainControl(
  #method = 'repeatedcv',
  method = "cv",
  number = 5,
  #repeats = 5
  savePredictions = "final",
  classProbs = T,
  summaryFunction = defaultSummary
)

```

## Multiple Linear Regression Model (MLM)
The data sets were intialled modelled using a standard multiple linear regression, both were then tuned to see if there was an improvement by using a stepwise linear model. The stepwise method for fitting a regression model is where each varaible is selected based on it signifincance. in relation to both wine dataset the stepwise tuning resulting in a very small increase to the adjusted R2 value. When comparing the adjusted R-sqaure values from both models we see that the linear model is better at representing the red wine data, resulting in an R-sqaure of 0.35. Where the R-sqaure value from the white wine data was 0.28. When applied to the testing data the red wine data returns an RMSE value of 0.68 and the white wine data returns 0.75. In combintaion of both the R-sqaure and RMSE values we can see the MLM is a better model for the red wine data. I dont think a linear model particularly suits either dataset as the R-sqaure values are quite low therfore suggesting the model does not represent the data well. something that could hinder the linear model is the reponse value being an interger, when the model is looking to predict it will use floats and therefore smooths out the discrmepanciey from one interger to another. It was also seen through the the scatter plots there was large amount of variance and no feature returned a visually strong correlation. 
```{r MLM}
library(MASS)
set.seed(345)

red.mlm_mdl <- train(quality ~ . , data = red_training, 
                    method = 'lm',
                    trControl = control_obj_regression,
                    trace = F)
red.mlm_mdl
summary(red.mlm_mdl)
# Insignificant values: fixed_acidity,citric_acid,residual_sugar,density

red.mlm_mdl.tuned <- train(quality ~ . , data = red_training, 
                method = 'lmStepAIC',
                trControl = control_obj_regression,
                trace = F)
red.mlm_mdl.tuned
summary(red.mlm_mdl.tuned)  

# applying to testing data
red_testing <-  red_testing %>% 
  mutate(predictions = predict(red.mlm_mdl.tuned, newdata = red_testing))

## calculting the RMSE amd R squared of the linear model on the testing data
red.mlm_mdl.tuned.metrics <- data.frame(
    dataset = 'Red Wine',
  RMSE =RMSE(pred = red_testing$predictions,
       obs = red_testing$quality),
  R2 = R2(pred = red_testing$predictions,
     obs = red_testing$quality)
)
## plotting the predictions against the actual score margin from the testing data
ggplot(red_testing, aes(predictions, quality)) +
  geom_point(colour = '#1111FF', alpha=0.7) +
  geom_abline(colour = "red", linetype = "dashed")

#-------------------------------------------------------------------------------------#
white.mlm_mdl <- train(quality ~ . , data = white_training, 
                      method = 'lm',
                      trControl = control_obj_regression,
                      trace = F)
white.mlm_mdl
summary(white.mlm_mdl)
# Insignificant values: citric_acid,chlorides,total_sulfur_dioxide 
white.mlm_mdl.tuned <- train(quality ~ . , data = white_training, 
                method = 'lmStepAIC',
                trControl = control_obj_regression,
                trace = F)
white.mlm_mdl.tuned
summary(white.mlm_mdl.tuned)  

# applying to testing data
white_testing <-  white_testing %>% 
  mutate(predictions = predict(white.mlm_mdl.tuned, newdata = white_testing))

## calculting the RMSE amd R squared of the linear model on the testing data
white.mlm_mdl.tuned.metrics <- data.frame(
  dataset = 'White Wine',
  RMSE = RMSE(pred = white_testing$predictions,
       obs = white_testing$quality),
  R2 = R2(pred = white_testing$predictions,
     obs = white_testing$quality)
)
## plotting the predictions against the actual score margin from the testing data
ggplot(white_testing, aes(predictions, quality)) +
  geom_point(colour = '#1111FF', alpha=0.7) +
  geom_abline(colour = "red", linetype = "dashed")

rbind(red.mlm_mdl.tuned.metrics, white.mlm_mdl.tuned.metrics)

detach("package:MASS")
rm(red.mlm_mdl,red.mlm_mdl.tuned,red.mlm_mdl.tuned.metrics,white.mlm_mdl,white.mlm_mdl.tuned,white.mlm_mdl.tuned.metrics)
```

## Penalised regression models 
```{r Penalised Models}
set.seed(345)
lambda <- 10^seq(-3,3,length=100)

# Ridge
red_ridge <- train(
  quality~., data=red_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=0, lambda=lambda)
)

coef(red_ridge$finalModel, red_ridge$bestTune$lambda)
predictions <- red_ridge %>% predict(red_testing)

data.frame(
  RMSE.rid.red = RMSE(predictions, red_testing$quality),
  Rsquare.rid.red = caret::R2(predictions, red_testing$quality)
)

#lasso
set.seed(123)
red_lasso <- train(
  quality~., data=red_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=1, lambda=lambda)
)
  
coef(red_lasso$finalModel, red_lasso$bestTune$lambda)
predictions <- red_lasso %>% predict(red_testing)

data.frame(
  RMSE.las.red = RMSE(predictions, red_testing$quality),
  Rsquare.las.red = caret::R2(predictions, red_testing$quality)
)

#elastic
set.seed(123)
red_elastic <- train(
  quality~., data=red_training, method="glmnet", 
  trControl=control_obj_regression,
)

coef(red_elastic$finalModel, red_elastic$bestTune$lambda)
predictions <- red_elastic %>% predict(red_testing)

data.frame(
  RMSE.net.red = RMSE(predictions, red_testing$quality),
  Rsquare.net.red = caret::R2(predictions, red_testing$quality)
)

#caret compare all of ridge, lasso, elastic net: 
#choose best with smallest median or mean RMSE
models <- list(red_ridge=red_ridge, red_lasso=red_lasso, red_elastic=red_elastic)
resamples(models) %>% summary(metric="RMSE")

#-------------------------------------------------------------------------------------#

# Ridge
white_ridge <- train(
  quality~., data=white_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=0, lambda=lambda)
)

coef(white_ridge$finalModel, white_ridge$bestTune$lambda)
predictions <- white_ridge %>% predict(white_testing)

data.frame(
  RMSE.rid.white = RMSE(predictions, white_testing$quality),
  Rsquare.rid.white = caret::R2(predictions, white_testing$quality)
)

#lasso
set.seed(123)
white_lasso <- train(
  quality~., data=white_training, method="glmnet", 
  trControl=control_obj_regression,
  tuneGrid=expand.grid(alpha=1, lambda=lambda)
)
  
coef(white_lasso$finalModel, white_lasso$bestTune$lambda)
predictions <- white_lasso %>% predict(white_testing)

data.frame(
  RMSE.las.white = RMSE(predictions, white_testing$quality),
  Rsquare.las.white = caret::R2(predictions, white_testing$quality)
)

#elastic
set.seed(123)
white_elastic <- train(
  quality~., data=white_training, method="glmnet", 
  trControl=control_obj_regression,
)

coef(white_elastic$finalModel, white_elastic$bestTune$lambda)
predictions <- white_elastic %>% predict(white_testing)

data.frame(
  RMSE.net.white = RMSE(predictions, white_testing$quality),
  Rsquare.net.white = R2(predictions, white_testing$quality)
)

#caret compare all of ridge, lasso, elastic net: 
#choose best with smallest median or mean RMSE
models <- list(red_ridge=red_ridge, red_lasso=red_lasso, red_elastic=red_elastic,
               white_ridge=white_ridge, white_lasso=white_lasso, white_elastic=white_elastic)
resamples(models) %>% summary(metric="RMSE")


```
## Decsion Tree Model
```{r DT}
set.seed(345)
library(rpart)
library(rpart.plot)


red_tree <-  rpart(quality ~.,
                    data = red_training,
                    method = "anova")
rpart.plot(red_tree)

predictions = predict(red_tree, newdata = red_testing)

red_tree.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

red_tree.tuned <- train(quality ~.,
                   data = red_training,
                   method = "rpart",
                   trControl = control_obj_regression,
                   tuneGrid = expand.grid(cp = seq(0.001,0.02,0.001)))
red_tree.tuned
plot(red_tree.tuned)
rattle::fancyRpartPlot(red_tree.tuned$finalModel, sub = "")
plot(varImp(red_tree.tuned))


predictions = predict(red_tree.tuned, newdata = red_testing)

red_tree.tuned.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

rbind(red_tree.metrics, red_tree.tuned.metrics)

#-------------------------------------------------------------------------------------#

white_tree <-  rpart(quality ~.,
                    data = white_training,
                    method = "anova")
rpart.plot(white_tree)

predictions = predict(white_tree, newdata = white_testing)

white_tree.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

white_tree.tuned <- train(quality ~.,
                   data = white_training,
                   method = "rpart",
                   trControl = control_obj_regression,
                   tuneGrid = expand.grid(cp = seq(0.001,0.02,0.001)))
white_tree.tuned
plot(white_tree.tuned)
rattle::fancyRpartPlot(white_tree.tuned$finalModel, sub = "")
plot(varImp(white_tree.tuned))


predictions = predict(white_tree.tuned, newdata = white_testing)

white_tree.tuned.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

rbind(white_tree.metrics, white_tree.tuned.metrics)

models <- list(red_tree.tuned=red_tree.tuned,
               white_tree.tuned=white_tree.tuned)
resamples(models) %>% summary(metric="RMSE")


```
## Random forest Model
```{r RF}
set.seed(345)
red.rf_mdl <- train(quality ~ ., data = red_training,
                method = 'rf',
                trControl = control_obj_regression)

red.rf_mdl
plot(red.rf_mdl)
plot(varImp(red.rf_mdl))

predictions = predict(red.rf_mdl, newdata = red_testing)

red_rf.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

#-------------------------------------------------------------------------------------#

white.rf_mdl <- train(quality ~ ., data = white_training,
                method = 'rf',
                trControl = control_obj_regression)

white.rf_mdl
plot(white.rf_mdl)
plot(varImp(white.rf_mdl))

predictions = predict(white.rf_mdl, newdata = white_testing)

white_rf.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)


models <- list(red.rf_mdl=red.rf_mdl,
               white.rf_mdl=white.rf_mdl)
resamples(models) %>% summary(metric="RMSE")


```
## K Nearest Neighbour Model
```{r KNN}
set.seed(345)

red.knn_mdl <-  train(quality~ ., data = red_training, method = "knn",
                  preProc = c('center', 'scale'),
                  trControl = control_obj_regression)
red.knn_mdl

predictions = predict(red.knn_mdl, newdata = red_testing)

red_knn.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

red.knn_mdl.tuned <-  train(quality~ ., data = red_training, method = "knn",
                  preProc = c('center', 'scale'),
                  tuneGrid = data.frame(.k = 1:50),
                  trControl = control_obj_regression)
red.knn_mdl.tuned

predictions = predict(red.knn_mdl.tuned, newdata = red_testing)

red_knn.tuned.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = red_testing$quality),
  R2 = R2(pred = predictions,obs = red_testing$quality)
)

#-------------------------------------------------------------------------------------#

white.knn_mdl <-  train(quality~ ., data = white_training, method = "knn",
                  preProc = c('center', 'scale'),
                  trControl = control_obj_regression)
white.knn_mdl

predictions = predict(white.knn_mdl, newdata = white_testing)

white_knn.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

white.knn_mdl.tuned <-  train(quality~ ., data = white_training, method = "knn",
                  preProc = c('center', 'scale'),
                  tuneGrid = data.frame(.k = 1:50),
                  trControl = control_obj_regression)
white.knn_mdl.tuned

predictions = predict(white.knn_mdl.tuned, newdata = white_testing)

white_knn.tuned.metrics <- data.frame(
  RMSE = RMSE(pred = predictions,obs = white_testing$quality),
  R2 = R2(pred = predictions,obs = white_testing$quality)
)

models <- list(red.knn_mdl=red.knn_mdl, red.knn_mdl.tuned = red.knn_mdl.tuned,
               white.knn_mdl=white.knn_mdl, white.knn_mdl.tuned = white.knn_mdl.tuned)
resamples(models) %>% summary(metric="RMSE")

```
# Classification Models
## Split Data For Classification
```{r RF Classification}
set.seed(345)

red_wine$quality <- as.factor(red_wine$quality)
red_wine$quality <- as.factor(red_wine$quality)

white_wine$quality <- as.factor(white_wine$quality)
white_wine$quality <- as.factor(white_wine$quality)

red_wine <- red_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))
red_wine <- red_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))
white_wine <- white_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))
white_wine <- white_wine %>% 
  mutate(quality = factor(quality, 
                        labels = make.names(levels(quality))))


inTrain_red <- createDataPartition(y = red_wine$quality, p = 0.8, list = F)

red_c_training <-  red_wine %>% 
  slice(inTrain_red)

red_c_testing <-  red_wine %>% 
  slice(-inTrain_red)

inTrain_white <- createDataPartition(y = white_wine$quality, p = 0.8, list = F)

white_c_training <-  white_wine %>% 
  slice(inTrain_white)

white_c_testing <-  white_wine %>% 
  slice(-inTrain_white)

```

## Random Forest Model
```{r RF Classification}
set.seed(345)

red_c_training %>% 
  group_by(quality) %>% 
  count()

white_c_training %>% 
  group_by(quality) %>% 
  count()


red_rf_mdl.C <- train(quality ~ . ,
                 data = red_training,
                 method = "rf",
                 trControl = control_obj_classification,
                 #tuneGrid = tune,
                 importance = TRUE, 

                 )
plot(red_rf_mdl.C)
plot(varImp(red_rf_mdl.C))


confusionMatrix(data = red_rf_mdl.C,
                reference = red_c_testing$quality)


white_rf_mdl.C <- train(quality ~ . ,
                 data = white_training,
                 method = "rf",
                 trControl = control_obj_classification,
                 #tuneGrid = tune,
                 importance = TRUE, 
                 )
plot(white_rf_mdl.C)
plot(varImp(white_rf_mdl.C))

confusionMatrix(data = white_rf_mdl.C,
                reference = white_c_testing$quality)


```

## Boosted Model
```{r Boosted}
set.seed(345)

red_gbtree <- train(quality ~.,
                data = red_c_training,
                method = "xgbTree",
                trControl = control_obj_classification)


plot(varImp(red_gbtree))

confusionMatrix(data = gbtree,
                reference = red_c_testing$quality)


white_gbtree <- train(quality ~.,
                data = white_c_training,
                method = "xgbTree",
                trControl = control_obj_classification)


plot(varImp(white_gbtree))

confusionMatrix(data = white_gbtree,
                reference = white_c_testing$quality)

plot(red_gbtree)
```

